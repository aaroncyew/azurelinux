From 47051e5b436bbf545aaff1314b15da825a93cbfb Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Fri, 26 Jun 2020 19:57:10 +0200
Subject: [PATCH 01/20] timing_runner: add ability to specify CPU affinity for
 tcpdump

---
 scripts/test-bleichenbacher-timing.py | 14 +++++++++++---
 scripts/test-lucky13.py               | 12 ++++++++++--
 tlsfuzzer/timing_runner.py            | 12 ++++++++++--
 3 files changed, 31 insertions(+), 7 deletions(-)

diff --git a/scripts/test-bleichenbacher-timing.py b/scripts/test-bleichenbacher-timing.py
index 5f5acead4..9be5325c2 100644
--- a/scripts/test-bleichenbacher-timing.py
+++ b/scripts/test-bleichenbacher-timing.py
@@ -24,7 +24,7 @@
 from tlslite.extensions import SNIExtension
 from tlsfuzzer.utils.lists import natural_sort_keys
 
-version = 1
+version = 2
 
 
 def help_msg():
@@ -67,6 +67,9 @@ def help_msg():
     print(" --no-sni       do not send server name extension.")
     print("                Sends extension by default if the hostname is a")
     print("                valid DNS name, not an IP address")
+    print(" --cpu-list     Set the CPU affinity for the tcpdump process")
+    print("                See taskset(1) man page for the syntax of this")
+    print("                option. Not used by default.")
     print(" --help         this message")
 
 
@@ -88,6 +91,7 @@ def main():
     timing = False
     outdir = "/tmp"
     cipher = CipherSuite.TLS_RSA_WITH_AES_128_CBC_SHA
+    affinity = None
 
     argv = sys.argv[1:]
     opts, args = getopt.getopt(argv,
@@ -95,7 +99,8 @@ def main():
                                ["help",
                                 "no-safe-renego",
                                 "no-sni",
-                                "repeat="])
+                                "repeat=",
+                                "cpu-list="])
     for opt, arg in opts:
         if opt == '-h':
             host = arg
@@ -137,6 +142,8 @@ def main():
             srv_extensions = None
         elif opt == "--no-sni":
             no_sni = True
+        elif opt == "--cpu-list":
+            affinity = arg
         elif opt == '--help':
             help_msg()
             sys.exit(0)
@@ -782,7 +789,8 @@ def main():
                                          outdir,
                                          host,
                                          port,
-                                         interface)
+                                         interface,
+                                         affinity)
             print("Running timing tests...")
             timing_runner.generate_log(run_only, run_exclude, repetitions)
             ret_val = timing_runner.run()
diff --git a/scripts/test-lucky13.py b/scripts/test-lucky13.py
index f7145e09c..3f12d0974 100644
--- a/scripts/test-lucky13.py
+++ b/scripts/test-lucky13.py
@@ -59,6 +59,9 @@ def help_msg():
     print(" --repeat rep   How many timing samples should be gathered for each test")
     print("                100 by default")
     print(" --quick        Only run a basic subset of tests")
+    print(" --cpu-list     Set the CPU affinity for the tcpdump process")
+    print("                See taskset(1) man page for the syntax of this")
+    print("                option. Not used by default.")
     print(" --help         this message")
 
 
@@ -75,11 +78,13 @@ def main():
     interface = None
     quick = False
     cipher = CipherSuite.TLS_RSA_WITH_AES_128_CBC_SHA
+    affinity = None
 
     argv = sys.argv[1:]
     opts, args = getopt.getopt(argv, "h:p:e:x:X:n:l:o:i:C:", ["help",
                                                               "repeat=",
-                                                              "quick"])
+                                                              "quick",
+                                                              "cpu-list="])
     for opt, arg in opts:
         if opt == '-h':
             host = arg
@@ -118,6 +123,8 @@ def main():
             sys.exit(0)
         elif opt == '--quick':
             quick = True
+        elif opt == '--cpu-list':
+            affinity = arg
         else:
             raise ValueError("Unknown option: {0}".format(opt))
 
@@ -446,7 +453,8 @@ def main():
                                              outdir,
                                              host,
                                              port,
-                                             interface)
+                                             interface,
+                                             affinity)
                 print("Running timing tests...")
                 timing_runner.generate_log(run_only, run_exclude, repetitions)
                 ret_val = timing_runner.run()
diff --git a/tlsfuzzer/timing_runner.py b/tlsfuzzer/timing_runner.py
index cea67aacb..f3cc957a8 100644
--- a/tlsfuzzer/timing_runner.py
+++ b/tlsfuzzer/timing_runner.py
@@ -20,7 +20,8 @@
 class TimingRunner:
     """Repeatedly runs tests and captures timing information."""
 
-    def __init__(self, name, tests, out_dir, ip_address, port, interface):
+    def __init__(self, name, tests, out_dir, ip_address, port, interface,
+                 affinity=None):
         """
         Check if tcpdump is present and setup instance parameters.
 
@@ -30,6 +31,9 @@ def __init__(self, name, tests, out_dir, ip_address, port, interface):
         :param str ip_address: Server IP address
         :param int port: Server port
         :param str interface: Network interface to run tcpdump on
+        :param str affinity: The processor IDs to use for affinity of
+            the `tcpdump` process. See taskset man page for description
+            of --cpu-list option.
         """
         # first check tcpdump presence
         if not self.check_tcpdump():
@@ -42,6 +46,7 @@ def __init__(self, name, tests, out_dir, ip_address, port, interface):
         self.port = port
         self.interface = interface
         self.log = Log(os.path.join(self.out_dir, "log.csv"))
+        self.affinity = affinity
 
         self.tcpdump_running = True
 
@@ -169,7 +174,10 @@ def sniff(self):
                  '--time-stamp-precision', 'nano']
 
         output_file = os.path.join(self.out_dir, "capture.pcap")
-        cmd = ['tcpdump', packet_filter, '-w', output_file] + flags
+        cmd = []
+        if self.affinity:
+            cmd += ['taskset', '--cpu-list', self.affinity]
+        cmd += ['tcpdump', packet_filter, '-w', output_file] + flags
         process = subprocess.Popen(cmd, stderr=subprocess.PIPE)
 
         # detect when tcpdump starts capturing

From bea21bf9d2f3b1ca10eee08df3edcc218edf76ac Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Fri, 26 Jun 2020 20:17:34 +0200
Subject: [PATCH 02/20] add new modules to docs

---
 docs/source/tlsfuzzer.analysis.rst      | 7 +++++++
 docs/source/tlsfuzzer.extract.rst       | 7 +++++++
 docs/source/tlsfuzzer.rst               | 3 +++
 docs/source/tlsfuzzer.timing_runner.rst | 7 +++++++
 4 files changed, 24 insertions(+)
 create mode 100644 docs/source/tlsfuzzer.analysis.rst
 create mode 100644 docs/source/tlsfuzzer.extract.rst
 create mode 100644 docs/source/tlsfuzzer.timing_runner.rst

diff --git a/docs/source/tlsfuzzer.analysis.rst b/docs/source/tlsfuzzer.analysis.rst
new file mode 100644
index 000000000..f3788a436
--- /dev/null
+++ b/docs/source/tlsfuzzer.analysis.rst
@@ -0,0 +1,7 @@
+tlsfuzzer.analysis module
+=========================
+
+.. automodule:: tlsfuzzer.analysis
+    :members:
+    :undoc-members:
+    :show-inheritance:
diff --git a/docs/source/tlsfuzzer.extract.rst b/docs/source/tlsfuzzer.extract.rst
new file mode 100644
index 000000000..22b0e1d05
--- /dev/null
+++ b/docs/source/tlsfuzzer.extract.rst
@@ -0,0 +1,7 @@
+tlsfuzzer.extract module
+========================
+
+.. automodule:: tlsfuzzer.extract
+    :members:
+    :undoc-members:
+    :show-inheritance:
diff --git a/docs/source/tlsfuzzer.rst b/docs/source/tlsfuzzer.rst
index be0d6ab90..3b4dd4d35 100644
--- a/docs/source/tlsfuzzer.rst
+++ b/docs/source/tlsfuzzer.rst
@@ -18,12 +18,15 @@ Submodules
 
 .. toctree::
 
+   tlsfuzzer.analysis
    tlsfuzzer.expect
+   tlsfuzzer.extract
    tlsfuzzer.fuzzers
    tlsfuzzer.handshake_helpers
    tlsfuzzer.helpers
    tlsfuzzer.messages
    tlsfuzzer.runner
    tlsfuzzer.scanner
+   tlsfuzzer.timing_runner
    tlsfuzzer.tree
 
diff --git a/docs/source/tlsfuzzer.timing_runner.rst b/docs/source/tlsfuzzer.timing_runner.rst
new file mode 100644
index 000000000..8efe5c55d
--- /dev/null
+++ b/docs/source/tlsfuzzer.timing_runner.rst
@@ -0,0 +1,7 @@
+tlsfuzzer.timing_runner module
+==============================
+
+.. automodule:: tlsfuzzer.timing_runner
+    :members:
+    :undoc-members:
+    :show-inheritance:

From 2278c404afbbcc727404e28d9271b43b2208cf79 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Mon, 29 Jun 2020 17:09:02 +0200
Subject: [PATCH 03/20] report progress when running timing tests

since running tests with large population sizes can take hours, if
not days, report progress of the process as it's running
---
 tlsfuzzer/timing_runner.py | 9 +++++++--
 tlsfuzzer/utils/log.py     | 4 ++++
 2 files changed, 11 insertions(+), 2 deletions(-)

diff --git a/tlsfuzzer/timing_runner.py b/tlsfuzzer/timing_runner.py
index f3cc957a8..03e97c68c 100644
--- a/tlsfuzzer/timing_runner.py
+++ b/tlsfuzzer/timing_runner.py
@@ -73,7 +73,7 @@ def generate_log(self, run_only, run_exclude, repetitions):
         self.log.start_log(actual_tests)
 
         # generate requested number of random order test runs
-        for _ in range(0, repetitions):
+        for _ in range(repetitions):
             self.log.shuffle_new_run()
 
         self.log.write()
@@ -92,9 +92,14 @@ def run(self):
         # run the conversations
         test_classes = self.log.get_classes()
         # prepend the conversations with few warm-up ones
+        exp_len = WARM_UP + sum(1 for _ in self.log.iterate_log())
+        self.log.read_log()
         queries = chain(repeat(0, WARM_UP), self.log.iterate_log())
         print("Starting timing info collection. This might take a while...")
-        for index in queries:
+        for executed, index in enumerate(queries):
+            if executed % 20 == 0:
+                print("Done: {0:6.2f}%".format(executed*100.0/exp_len),
+                      end="\r")
             if self.tcpdump_running:
                 c_name = test_classes[index]
                 c_test = self.tests[c_name]
diff --git a/tlsfuzzer/utils/log.py b/tlsfuzzer/utils/log.py
index 0dc132a5a..a92bc2889 100644
--- a/tlsfuzzer/utils/log.py
+++ b/tlsfuzzer/utils/log.py
@@ -49,6 +49,8 @@ def start_log(self, class_list):
         :param list class_list: List of test classes to be used to identify a
             connection later during the analysis
         """
+        if self.logfile:
+            self.logfile.close()
         self.logfile = open(self.filename, 'w')
         self.classes = class_list
         self.writer = csv.writer(self.logfile)
@@ -60,6 +62,8 @@ def write(self):
 
     def read_log(self):
         """Read classes from log file into internal list."""
+        if self.logfile:
+            self.logfile.close()
         self.logfile = open(self.filename, 'r')
         self.reader = csv.reader(self.logfile)
         self.classes = next(self.reader)

From bd0143990ac65c13f79a2e206ad040a233a5fb67 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Mon, 29 Jun 2020 17:35:45 +0200
Subject: [PATCH 04/20] write also a legend for the generated graphs

since the assignment of ids to test names is random by design, it's
hard to find which IDs (class numbers) correspond to which tests,
this creates a simple legend file that puts that data in easy to
read and easy to find file
---
 tests/test_tlsfuzzer_analysis.py |  6 ++++--
 tlsfuzzer/analysis.py            | 11 +++++++++--
 2 files changed, 13 insertions(+), 4 deletions(-)

diff --git a/tests/test_tlsfuzzer_analysis.py b/tests/test_tlsfuzzer_analysis.py
index ab652be29..27a24e1dd 100644
--- a/tests/test_tlsfuzzer_analysis.py
+++ b/tests/test_tlsfuzzer_analysis.py
@@ -60,7 +60,8 @@ def test_report(self):
                                 mock_ecdf.assert_called_once()
                                 mock_box.assert_called_once()
                                 mock_scatter.assert_called_once()
-                                mock_open.assert_called_once()
+                                # we're writing both report.csv and legend.csv
+                                self.assertEqual(mock_open.call_count, 2)
                                 self.assertEqual(ret, 0)
 
     def test_report_neq(self):
@@ -81,7 +82,8 @@ def test_report_neq(self):
                                 mock_ecdf.assert_called_once()
                                 mock_box.assert_called_once()
                                 mock_scatter.assert_called_once()
-                                mock_open.assert_called_once()
+                                # we're writing both report.csv and legend.csv
+                                self.assertEqual(mock_open.call_count, 2)
                                 self.assertEqual(ret, 1)
 
     def test_ks_test(self):
diff --git a/tlsfuzzer/analysis.py b/tlsfuzzer/analysis.py
index a3acc57d4..180347696 100644
--- a/tlsfuzzer/analysis.py
+++ b/tlsfuzzer/analysis.py
@@ -250,8 +250,15 @@ def generate_report(self):
                        ks_results[pair]
                        ]
                 writer.writerow(row)
-            print("For detailed report see {}".format(report_filename))
-            return difference
+        legend_filename = join(self.output, "legend.csv")
+        with open(legend_filename, "w") as csv_file:
+            writer = csv.writer(csv_file)
+            writer.writerow(['ID', 'Name'])
+            for num, name in enumerate(self.class_names):
+                writer.writerow([num, name])
+
+        print("For detailed report see {}".format(report_filename))
+        return difference
 
 
 if __name__ == '__main__':

From 0a9467e3136219907e9d16392d08c38cc28bc189 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Mon, 29 Jun 2020 17:40:30 +0200
Subject: [PATCH 05/20] stop tcpdump on test cancellation

When the script is killed (by any reason, including Ctrl+C), the
tcpdump process will be left running

this fixes this issue
---
 tlsfuzzer/timing_runner.py | 71 ++++++++++++++++++++------------------
 1 file changed, 37 insertions(+), 34 deletions(-)

diff --git a/tlsfuzzer/timing_runner.py b/tlsfuzzer/timing_runner.py
index 03e97c68c..50f2ca965 100644
--- a/tlsfuzzer/timing_runner.py
+++ b/tlsfuzzer/timing_runner.py
@@ -89,40 +89,43 @@ def run(self):
         status.setDaemon(True)
         status.start()
 
-        # run the conversations
-        test_classes = self.log.get_classes()
-        # prepend the conversations with few warm-up ones
-        exp_len = WARM_UP + sum(1 for _ in self.log.iterate_log())
-        self.log.read_log()
-        queries = chain(repeat(0, WARM_UP), self.log.iterate_log())
-        print("Starting timing info collection. This might take a while...")
-        for executed, index in enumerate(queries):
-            if executed % 20 == 0:
-                print("Done: {0:6.2f}%".format(executed*100.0/exp_len),
-                      end="\r")
-            if self.tcpdump_running:
-                c_name = test_classes[index]
-                c_test = self.tests[c_name]
-
-                runner = Runner(c_test)
-                res = True
-                try:
-                    runner.run()
-                except Exception:
-                    print("Error while processing")
-                    print(traceback.format_exc())
-                    res = False
-
-                if not res:
-                    raise AssertionError("Test must pass in order to be timed")
-            else:
-                sys.exit(1)
-
-        # stop sniffing and give tcpdump time to write all buffered packets
-        self.tcpdump_running = False
-        time.sleep(2)
-        sniffer.terminate()
-        sniffer.wait()
+        try:
+            # run the conversations
+            test_classes = self.log.get_classes()
+            # prepend the conversations with few warm-up ones
+            exp_len = WARM_UP + sum(1 for _ in self.log.iterate_log())
+            self.log.read_log()
+            queries = chain(repeat(0, WARM_UP), self.log.iterate_log())
+            print("Starting timing info collection. "
+                  "This might take a while...")
+            for executed, index in enumerate(queries):
+                if executed % 20 == 0:
+                    print("Done: {0:6.2f}%".format(executed*100.0/exp_len),
+                          end="\r")
+                if self.tcpdump_running:
+                    c_name = test_classes[index]
+                    c_test = self.tests[c_name]
+
+                    runner = Runner(c_test)
+                    res = True
+                    try:
+                        runner.run()
+                    except Exception:
+                        print("Error while processing")
+                        print(traceback.format_exc())
+                        res = False
+
+                    if not res:
+                        raise AssertionError(
+                            "Test must pass in order to be timed")
+                else:
+                    sys.exit(1)
+        finally:
+            # stop sniffing and give tcpdump time to write all buffered packets
+            self.tcpdump_running = False
+            time.sleep(2)
+            sniffer.terminate()
+            sniffer.wait()
 
         # start extraction and analysis
         print("Starting extraction...")

From 7605cb5888b2e48b5a992792d113ba0ee6ce4675 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Tue, 30 Jun 2020 18:32:51 +0200
Subject: [PATCH 06/20] bleichebnacher-timing: make order of tests
 deterministic, speed up

it's not necessary to send the AppData, the server is expected to
reject the Finished message

since the timing test randomises the the executed tests, it's not
necessary to randomise it before that (and it makes comparing
results from two runs harder)
---
 scripts/test-bleichenbacher-timing.py | 145 ++++++++++++++------------
 1 file changed, 77 insertions(+), 68 deletions(-)

diff --git a/scripts/test-bleichenbacher-timing.py b/scripts/test-bleichenbacher-timing.py
index 9be5325c2..1a6c1a215 100644
--- a/scripts/test-bleichenbacher-timing.py
+++ b/scripts/test-bleichenbacher-timing.py
@@ -13,7 +13,8 @@
 from tlsfuzzer.messages import Connect, ClientHelloGenerator, \
     ClientKeyExchangeGenerator, ChangeCipherSpecGenerator, \
     FinishedGenerator, ApplicationDataGenerator, AlertGenerator, \
-    TCPBufferingEnable, TCPBufferingDisable, TCPBufferingFlush, fuzz_mac
+    TCPBufferingEnable, TCPBufferingDisable, TCPBufferingFlush, fuzz_mac, \
+    fuzz_padding
 from tlsfuzzer.expect import ExpectServerHello, ExpectCertificate, \
     ExpectServerHelloDone, ExpectChangeCipherSpec, ExpectFinished, \
     ExpectAlert, ExpectClose, ExpectApplicationData, ExpectNoMessage
@@ -23,8 +24,9 @@
 from tlslite.utils.dns_utils import is_valid_hostname
 from tlslite.extensions import SNIExtension
 from tlsfuzzer.utils.lists import natural_sort_keys
+from tlsfuzzer.utils.ordered_dict import OrderedDict
 
-version = 2
+version = 3
 
 
 def help_msg():
@@ -77,7 +79,7 @@ def main():
     """Check if server is not vulnerable to Bleichenbacher attack"""
     host = "localhost"
     port = 4433
-    num_limit = 50
+    num_limit = None
     run_exclude = set()
     expected_failures = {}
     last_exp_tmp = None
@@ -165,7 +167,7 @@ def main():
         print("Ciphersuite has to use RSA key exchange.")
         exit(1)
 
-    conversations = {}
+    conversations = OrderedDict()
 
     conversation = Connect(host, port)
     node = conversation
@@ -224,6 +226,73 @@ def main():
 
     conversations["sanity - static non-zero byte in random padding"] = conversation
 
+    # first put tests that are the benchmark to be compared to
+    # fuzz MAC in the Finshed message to make decryption fail
+    conversation = Connect(host, port)
+    node = conversation
+    ciphers = [cipher]
+    node = node.add_child(ClientHelloGenerator(ciphers,
+                                               extensions=cln_extensions))
+    node = node.add_child(ExpectServerHello(extensions=srv_extensions))
+
+    node = node.add_child(ExpectCertificate())
+    node = node.add_child(ExpectServerHelloDone())
+    node = node.add_child(TCPBufferingEnable())
+    node = node.add_child(ClientKeyExchangeGenerator())
+    node = node.add_child(ChangeCipherSpecGenerator())
+    node = node.add_child(fuzz_mac(FinishedGenerator(), xors={0:0xff}))
+    node = node.add_child(TCPBufferingDisable())
+    node = node.add_child(TCPBufferingFlush())
+    node = node.add_child(ExpectAlert(level,
+                                      alert))
+    node.add_child(ExpectClose())
+
+    conversations["invalid MAC in Finished on pos 0"] = conversation
+
+    conversation = Connect(host, port)
+    node = conversation
+    ciphers = [cipher]
+    node = node.add_child(ClientHelloGenerator(ciphers,
+                                               extensions=cln_extensions))
+    node = node.add_child(ExpectServerHello(extensions=srv_extensions))
+
+    node = node.add_child(ExpectCertificate())
+    node = node.add_child(ExpectServerHelloDone())
+    node = node.add_child(TCPBufferingEnable())
+    node = node.add_child(ClientKeyExchangeGenerator())
+    node = node.add_child(ChangeCipherSpecGenerator())
+    node = node.add_child(fuzz_mac(FinishedGenerator(), xors={-1:0xff}))
+    node = node.add_child(TCPBufferingDisable())
+    node = node.add_child(TCPBufferingFlush())
+    node = node.add_child(ExpectAlert(level,
+                                      alert))
+    node.add_child(ExpectClose())
+
+    conversations["invalid MAC in Finished on pos -1"] = conversation
+
+    # and for good measure, add something that sends invalid padding
+    conversation = Connect(host, port)
+    node = conversation
+    ciphers = [cipher]
+    node = node.add_child(ClientHelloGenerator(ciphers,
+                                               extensions=cln_extensions))
+    node = node.add_child(ExpectServerHello(extensions=srv_extensions))
+
+    node = node.add_child(ExpectCertificate())
+    node = node.add_child(ExpectServerHelloDone())
+    node = node.add_child(TCPBufferingEnable())
+    node = node.add_child(ClientKeyExchangeGenerator())
+    node = node.add_child(ChangeCipherSpecGenerator())
+    node = node.add_child(fuzz_padding(FinishedGenerator(),
+                                       xors={-1:0xff, -2:0x01}))
+    node = node.add_child(TCPBufferingDisable())
+    node = node.add_child(TCPBufferingFlush())
+    node = node.add_child(ExpectAlert(level,
+                                      alert))
+    node.add_child(ExpectClose())
+
+    conversations["invalid padding_length in Finished"] = conversation
+
     # set 2nd byte of padding to 3 (invalid value)
     conversation = Connect(host, port)
     node = conversation
@@ -238,7 +307,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(padding_subs={1: 3}))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -261,7 +329,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(padding_subs={1: 1}))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -284,7 +351,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(padding_subs={4: 0}))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -307,7 +373,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(padding_subs={-2: 0}))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -330,7 +395,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(padding_subs={2: 0}))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -353,7 +417,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(padding_subs={0: 1}))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -376,7 +439,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(padding_subs={-1: 1}))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -401,7 +463,6 @@ def main():
                                                      premaster_secret=bytearray([1] * 48)))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -424,7 +485,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(premaster_secret=bytearray([1, 1])))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -450,7 +510,6 @@ def main():
                                                      premaster_secret=bytearray([1, 1, 0])))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -476,7 +535,6 @@ def main():
                                                      premaster_secret=bytearray([1, 1, 0, 3])))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -499,7 +557,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(premaster_secret=bytearray([1] * 47)))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -522,7 +579,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(premaster_secret=bytearray([1] * 4)))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -545,7 +601,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(premaster_secret=bytearray([1] * 49)))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -568,7 +623,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(client_version=(2, 2)))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -591,7 +645,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(client_version=(0, 0)))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -617,7 +670,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(padding_subs={1: 0, 2: 2}))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -643,7 +695,6 @@ def main():
     node = node.add_child(ClientKeyExchangeGenerator(padding_subs={0: 2}))
     node = node.add_child(ChangeCipherSpecGenerator())
     node = node.add_child(FinishedGenerator())
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
     node = node.add_child(TCPBufferingDisable())
     node = node.add_child(TCPBufferingFlush())
     node = node.add_child(ExpectAlert(level,
@@ -652,51 +703,6 @@ def main():
 
     conversations["too long PKCS padding"] = conversation
 
-    # fuzz MAC in the Finshed message to make decryption fail
-    conversation = Connect(host, port)
-    node = conversation
-    ciphers = [cipher]
-    node = node.add_child(ClientHelloGenerator(ciphers,
-                                               extensions=cln_extensions))
-    node = node.add_child(ExpectServerHello(extensions=srv_extensions))
-
-    node = node.add_child(ExpectCertificate())
-    node = node.add_child(ExpectServerHelloDone())
-    node = node.add_child(TCPBufferingEnable())
-    node = node.add_child(ClientKeyExchangeGenerator())
-    node = node.add_child(ChangeCipherSpecGenerator())
-    node = node.add_child(fuzz_mac(FinishedGenerator(), xors={0:0xff}))
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
-    node = node.add_child(TCPBufferingDisable())
-    node = node.add_child(TCPBufferingFlush())
-    node = node.add_child(ExpectAlert(level,
-                                      alert))
-    node.add_child(ExpectClose())
-
-    conversations["invalid MAC in Finished on pos 0"] = conversation
-
-    conversation = Connect(host, port)
-    node = conversation
-    ciphers = [cipher]
-    node = node.add_child(ClientHelloGenerator(ciphers,
-                                               extensions=cln_extensions))
-    node = node.add_child(ExpectServerHello(extensions=srv_extensions))
-
-    node = node.add_child(ExpectCertificate())
-    node = node.add_child(ExpectServerHelloDone())
-    node = node.add_child(TCPBufferingEnable())
-    node = node.add_child(ClientKeyExchangeGenerator())
-    node = node.add_child(ChangeCipherSpecGenerator())
-    node = node.add_child(fuzz_mac(FinishedGenerator(), xors={-1:0xff}))
-    node = node.add_child(ApplicationDataGenerator(bytearray(b"GET / HTTP/1.0\r\n\r\n")))
-    node = node.add_child(TCPBufferingDisable())
-    node = node.add_child(TCPBufferingFlush())
-    node = node.add_child(ExpectAlert(level,
-                                      alert))
-    node.add_child(ExpectClose())
-
-    conversations["invalid MAC in Finished on pos -1"] = conversation
-
     # run the conversation
     good = 0
     bad = 0
@@ -717,7 +723,10 @@ def main():
     else:
         regular_tests = [(k, v) for k, v in conversations.items() if
                          (k != 'sanity') and k not in run_exclude]
-    sampled_tests = sample(regular_tests, min(num_limit, len(regular_tests)))
+    if num_limit < len(conversations):
+        sampled_tests = sample(regular_tests, min(num_limit, len(regular_tests)))
+    else:
+        sampled_tests = regular_tests
     ordered_tests = chain(sanity_tests, sampled_tests, sanity_tests)
 
     print("Running tests for {0}".format(CipherSuite.ietfNames[cipher]))

From 18e9bc4a750f467ddd4529ec88361622bc4946be Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Thu, 2 Jul 2020 16:30:26 +0200
Subject: [PATCH 07/20] switch to Wilcoxon signed-rank test

because we collect observations in sets and the collection happens
over long periods of time (so noise that is constant for
a significant portion of collection will be visible in a portion of
observations), it means that our samples *are not* independent

because KS-test requires independent samples, we can't use it and
expect correct results (in practice, KS-test overestimates p-values
for dependent samples). Wilcoxon signed-rank test does work with
dependent samples. The downside is that it won't see change in variance
between samples. The upside is that it is significantly more sensitive
to changes in expected value.
---
 tests/test_tlsfuzzer_analysis.py |  8 ++++----
 tlsfuzzer/analysis.py            | 20 ++++++++++----------
 2 files changed, 14 insertions(+), 14 deletions(-)

diff --git a/tests/test_tlsfuzzer_analysis.py b/tests/test_tlsfuzzer_analysis.py
index 27a24e1dd..1c9c54ebd 100644
--- a/tests/test_tlsfuzzer_analysis.py
+++ b/tests/test_tlsfuzzer_analysis.py
@@ -25,11 +25,11 @@
 class TestReport(unittest.TestCase):
     def setUp(self):
         data = {
-            0: ["A", 0.000758130, 0.000696718, 0.000980080, 0.000988899, 0.000875510,
+            0: ["A", 0.000758129, 0.000696719, 0.000980079, 0.000988900, 0.000875509,
                 0.000734843, 0.000754852, 0.000667378, 0.000671230, 0.000790935],
             1: ["B", 0.000758130, 0.000696718, 0.000980080, 0.000988899, 0.000875510,
                 0.000734843, 0.000754852, 0.000667378, 0.000671230, 0.000790935],
-            2: ["C", 0.000758130, 0.000696718, 0.000980080, 0.000988899, 0.000875510,
+            2: ["C", 0.000758131, 0.000696717, 0.000980081, 0.000988898, 0.000875511,
                 0.000734843, 0.000754852, 0.000667378, 0.000671230, 0.000790935]
         }
         self.neq_data = {
@@ -91,10 +91,10 @@ def test_ks_test(self):
             analysis = Analysis("/tmp")
             self.mock_read_csv.assert_called_once()
 
-            res = analysis.ks_test()
+            res = analysis.wilcoxon_test()
             self.assertEqual(len(res), 3)
             for index, result in res.items():
-                self.assertEqual(result, 1.0)
+                self.assertGreaterEqual(result, 0.25)
 
     def test_box_test(self):
         with mock.patch("tlsfuzzer.analysis.pd.read_csv", self.mock_read_csv):
diff --git a/tlsfuzzer/analysis.py b/tlsfuzzer/analysis.py
index 180347696..81f26fe56 100644
--- a/tlsfuzzer/analysis.py
+++ b/tlsfuzzer/analysis.py
@@ -106,14 +106,14 @@ def box_test(self):
             results[TestPair(index1, index2)] = result
         return results
 
-    def ks_test(self):
-        """Cross-test all classes with the KS test"""
+    def wilcoxon_test(self):
+        """Cross-test all classes with the Wilcoxon signed-rank test"""
         results = {}
         comb = combinations(list(range(len(self.class_names))), 2)
         for index1, index2 in comb:
             data1 = self.data.iloc[:, index1]
             data2 = self.data.iloc[:, index2]
-            _, pval = stats.ks_2samp(data1, data2)
+            _, pval = stats.wilcoxon(data1, data2)
             results[TestPair(index1, index2)] = pval
         return results
 
@@ -216,12 +216,13 @@ def generate_report(self):
 
         # create a report with statistical tests
         box_results = self.box_test()
-        ks_results = self.ks_test()
+        wilcox_results = self.wilcoxon_test()
 
         report_filename = join(self.output, "report.csv")
         with open(report_filename, 'w') as file:
             writer = csv.writer(file)
-            writer.writerow(["Class 1", "Class 2", "Box test", "KS test"])
+            writer.writerow(["Class 1", "Class 2", "Box test",
+                             "Wilcoxon signed-rank test"])
             for pair, result in box_results.items():
                 index1 = pair.index1
                 index2 = pair.index2
@@ -236,18 +237,17 @@ def generate_report(self):
                 else:
                     print("Box test {} vs {}: No difference".format(index1,
                                                                     index2))
-                print("KS test {} vs {}: {}".format(index1,
-                                                    index2,
-                                                    ks_results[pair]))
+                print("Wilcoxon signed-rank test {} vs {}: {}"
+                      .format(index1, index2, wilcox_results[pair]))
                 # if both tests found a difference
                 # consider it a possible side-channel
-                if result and ks_results[pair] < 0.05:
+                if result and wilcox_results[pair] < 0.05:
                     difference = 1
 
                 row = [self.class_names[index1],
                        self.class_names[index2],
                        box_write,
-                       ks_results[pair]
+                       wilcox_results[pair]
                        ]
                 writer.writerow(row)
         legend_filename = join(self.output, "legend.csv")

From f2e067ad6ec54f48906d2baf7439a4636ecdf530 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Thu, 2 Jul 2020 18:55:11 +0200
Subject: [PATCH 08/20] Add second order test for Wilcoxon - uniformity of
 p-values

since it's normal to observe multiple false positives with
200 test executed (like with the Bleichenbacher), we should rather
look at overall distribution of p-values of the Wilcoxon test to decide
if the small p-values match expected frequency or not
---
 scripts/test-bleichenbacher-timing.py | 10 ++++++++--
 tlsfuzzer/analysis.py                 | 11 +++++++++++
 2 files changed, 19 insertions(+), 2 deletions(-)

diff --git a/scripts/test-bleichenbacher-timing.py b/scripts/test-bleichenbacher-timing.py
index 1a6c1a215..1721721cb 100644
--- a/scripts/test-bleichenbacher-timing.py
+++ b/scripts/test-bleichenbacher-timing.py
@@ -26,7 +26,7 @@
 from tlsfuzzer.utils.lists import natural_sort_keys
 from tlsfuzzer.utils.ordered_dict import OrderedDict
 
-version = 3
+version = 4
 
 
 def help_msg():
@@ -803,7 +803,13 @@ def main():
             print("Running timing tests...")
             timing_runner.generate_log(run_only, run_exclude, repetitions)
             ret_val = timing_runner.run()
-            print("Statistical analysis exited with {0}".format(ret_val))
+            if ret_val == 0:
+                print("No statistically significant difference detected")
+            elif ret_val == 1:
+                print("Statisticaly significant difference detected at alpha="
+                      "0.05")
+            else:
+                print("Statistical analysis exited with {0}".format(ret_val))
         else:
             print("Could not run timing tests because tcpdump is not present!")
             sys.exit(1)
diff --git a/tlsfuzzer/analysis.py b/tlsfuzzer/analysis.py
index 81f26fe56..6bc497627 100644
--- a/tlsfuzzer/analysis.py
+++ b/tlsfuzzer/analysis.py
@@ -219,6 +219,7 @@ def generate_report(self):
         wilcox_results = self.wilcoxon_test()
 
         report_filename = join(self.output, "report.csv")
+        p_vals = []
         with open(report_filename, 'w') as file:
             writer = csv.writer(file)
             writer.writerow(["Class 1", "Class 2", "Box test",
@@ -250,6 +251,9 @@ def generate_report(self):
                        wilcox_results[pair]
                        ]
                 writer.writerow(row)
+
+                p_vals.append(wilcox_results[pair])
+
         legend_filename = join(self.output, "legend.csv")
         with open(legend_filename, "w") as csv_file:
             writer = csv.writer(csv_file)
@@ -257,6 +261,13 @@ def generate_report(self):
             for num, name in enumerate(self.class_names):
                 writer.writerow([num, name])
 
+        _, p = stats.kstest(p_vals, 'uniform')
+        print("KS-test for uniformity of p-values from Wilcoxon signed-rank "
+              "test")
+        print("p-value: {}".format(p))
+        if p < 0.05:
+            difference = 1
+
         print("For detailed report see {}".format(report_filename))
         return difference
 

From 2481576b3590cfbc3cb24607efd68e353b013f1b Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Wed, 8 Jul 2020 18:46:09 +0200
Subject: [PATCH 09/20] bleichenbacher-timing: add tests with very long PMS
 values

we have tests with very short pre-master secret values (0, 1 and 2 byte
long), and while we have also a test with slightly larger PMS
(49-byte long), we don't have a test with very long PMS

Add tests with a PMS twice as long as a normal one and also one
that is as large as a 1024 bit RSA key can handle (124-byte long)
---
 scripts/test-bleichenbacher-timing.py | 45 ++++++++++++++++++++++++++-
 1 file changed, 44 insertions(+), 1 deletion(-)

diff --git a/scripts/test-bleichenbacher-timing.py b/scripts/test-bleichenbacher-timing.py
index 1721721cb..6565f2397 100644
--- a/scripts/test-bleichenbacher-timing.py
+++ b/scripts/test-bleichenbacher-timing.py
@@ -26,7 +26,7 @@
 from tlsfuzzer.utils.lists import natural_sort_keys
 from tlsfuzzer.utils.ordered_dict import OrderedDict
 
-version = 4
+version = 5
 
 
 def help_msg():
@@ -609,6 +609,49 @@ def main():
 
     conversations["too long (49-byte) pre master secret"] = conversation
 
+    # check if very long PMS is detected
+    conversation = Connect(host, port)
+    node = conversation
+    ciphers = [cipher]
+    node = node.add_child(ClientHelloGenerator(ciphers,
+                                               extensions=cln_extensions))
+    node = node.add_child(ExpectServerHello(extensions=srv_extensions))
+
+    node = node.add_child(ExpectCertificate())
+    node = node.add_child(ExpectServerHelloDone())
+    node = node.add_child(TCPBufferingEnable())
+    node = node.add_child(ClientKeyExchangeGenerator(premaster_secret=bytearray([1] * 124)))
+    node = node.add_child(ChangeCipherSpecGenerator())
+    node = node.add_child(FinishedGenerator())
+    node = node.add_child(TCPBufferingDisable())
+    node = node.add_child(TCPBufferingFlush())
+    node = node.add_child(ExpectAlert(level,
+                                      alert))
+    node.add_child(ExpectClose())
+
+    conversations["very long (124-byte) pre master secret"] = conversation
+
+    conversation = Connect(host, port)
+    node = conversation
+    ciphers = [cipher]
+    node = node.add_child(ClientHelloGenerator(ciphers,
+                                               extensions=cln_extensions))
+    node = node.add_child(ExpectServerHello(extensions=srv_extensions))
+
+    node = node.add_child(ExpectCertificate())
+    node = node.add_child(ExpectServerHelloDone())
+    node = node.add_child(TCPBufferingEnable())
+    node = node.add_child(ClientKeyExchangeGenerator(premaster_secret=bytearray([1] * 96)))
+    node = node.add_child(ChangeCipherSpecGenerator())
+    node = node.add_child(FinishedGenerator())
+    node = node.add_child(TCPBufferingDisable())
+    node = node.add_child(TCPBufferingFlush())
+    node = node.add_child(ExpectAlert(level,
+                                      alert))
+    node.add_child(ExpectClose())
+
+    conversations["very long (96-byte) pre master secret"] = conversation
+
     # check if wrong TLS version number is rejected
     conversation = Connect(host, port)
     node = conversation

From 541b034df7b1e44ee9bcbf27848db3892c292825 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Wed, 8 Jul 2020 18:54:35 +0200
Subject: [PATCH 10/20] extract: deterministic probe ordering in report

As it's normal to verify if the results are reproducible by running
the same test twice (or to compare results before and after
a change), make it easier by ensuring that the order of data on
graphs is constant.
---
 tlsfuzzer/extract.py | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/tlsfuzzer/extract.py b/tlsfuzzer/extract.py
index 9435efddf..807528fba 100644
--- a/tlsfuzzer/extract.py
+++ b/tlsfuzzer/extract.py
@@ -16,6 +16,7 @@
 
 from tlsfuzzer.utils.log import Log
 from tlsfuzzer.utils.statics import WARM_UP
+from tlsfuzzer.utils.lists import natural_sort_keys
 
 
 def help_msg():
@@ -144,7 +145,7 @@ def write_csv(self, filename):
         with open(filename, 'w') as csvfile:
             print("Writing to {0}".format(filename))
             writer = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)
-            for class_name in self.timings:
+            for class_name in sorted(self.timings, key=natural_sort_keys):
                 row = [class_name]
                 row.extend(self.timings[class_name])
                 writer.writerow(row)

From abe7870d0a038fb78a0902c88634bb4728c59bca Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Wed, 8 Jul 2020 19:35:48 +0200
Subject: [PATCH 11/20] analysis: remove Q-Q plot generation

The plots are not really informative with large or noisy samples,
it is also not really possible to generate it for the full lucky13
test as it requires generating over 200k individual plots
---
 tests/test_tlsfuzzer_analysis.py | 65 ++++++++++++++------------------
 tlsfuzzer/analysis.py            | 39 +------------------
 2 files changed, 29 insertions(+), 75 deletions(-)

diff --git a/tests/test_tlsfuzzer_analysis.py b/tests/test_tlsfuzzer_analysis.py
index 1c9c54ebd..1a073aa00 100644
--- a/tests/test_tlsfuzzer_analysis.py
+++ b/tests/test_tlsfuzzer_analysis.py
@@ -47,44 +47,40 @@ def setUp(self):
 
     def test_report(self):
         with mock.patch("tlsfuzzer.analysis.pd.read_csv", self.mock_read_csv):
-            with mock.patch("tlsfuzzer.analysis.Analysis.qq_plot") as mock_qq:
-                with mock.patch("tlsfuzzer.analysis.Analysis.ecdf_plot") as mock_ecdf:
-                    with mock.patch("tlsfuzzer.analysis.Analysis.box_plot") as mock_box:
-                        with mock.patch("tlsfuzzer.analysis.Analysis.scatter_plot") as mock_scatter:
-                            with mock.patch("__main__.__builtins__.open", mock.mock_open()) as mock_open:
-                                analysis = Analysis("/tmp")
-                                ret = analysis.generate_report()
-
-                                self.mock_read_csv.assert_called_once()
-                                mock_qq.assert_called_once()
-                                mock_ecdf.assert_called_once()
-                                mock_box.assert_called_once()
-                                mock_scatter.assert_called_once()
-                                # we're writing both report.csv and legend.csv
-                                self.assertEqual(mock_open.call_count, 2)
-                                self.assertEqual(ret, 0)
+            with mock.patch("tlsfuzzer.analysis.Analysis.ecdf_plot") as mock_ecdf:
+                with mock.patch("tlsfuzzer.analysis.Analysis.box_plot") as mock_box:
+                    with mock.patch("tlsfuzzer.analysis.Analysis.scatter_plot") as mock_scatter:
+                        with mock.patch("__main__.__builtins__.open", mock.mock_open()) as mock_open:
+                            analysis = Analysis("/tmp")
+                            ret = analysis.generate_report()
+
+                            self.mock_read_csv.assert_called_once()
+                            mock_ecdf.assert_called_once()
+                            mock_box.assert_called_once()
+                            mock_scatter.assert_called_once()
+                            # we're writing both report.csv and legend.csv
+                            self.assertEqual(mock_open.call_count, 2)
+                            self.assertEqual(ret, 0)
 
     def test_report_neq(self):
         timings = pd.DataFrame(data=self.neq_data)
         mock_read_csv = mock.Mock(spec=pd.read_csv)
         mock_read_csv.return_value = timings.transpose()
         with mock.patch("tlsfuzzer.analysis.pd.read_csv", mock_read_csv):
-            with mock.patch("tlsfuzzer.analysis.Analysis.qq_plot") as mock_qq:
-                with mock.patch("tlsfuzzer.analysis.Analysis.ecdf_plot") as mock_ecdf:
-                    with mock.patch("tlsfuzzer.analysis.Analysis.box_plot") as mock_box:
-                        with mock.patch("tlsfuzzer.analysis.Analysis.scatter_plot") as mock_scatter:
-                            with mock.patch("__main__.__builtins__.open", mock.mock_open()) as mock_open:
-                                analysis = Analysis("/tmp")
-                                ret = analysis.generate_report()
-
-                                mock_read_csv.assert_called_once()
-                                mock_qq.assert_called_once()
-                                mock_ecdf.assert_called_once()
-                                mock_box.assert_called_once()
-                                mock_scatter.assert_called_once()
-                                # we're writing both report.csv and legend.csv
-                                self.assertEqual(mock_open.call_count, 2)
-                                self.assertEqual(ret, 1)
+            with mock.patch("tlsfuzzer.analysis.Analysis.ecdf_plot") as mock_ecdf:
+                with mock.patch("tlsfuzzer.analysis.Analysis.box_plot") as mock_box:
+                    with mock.patch("tlsfuzzer.analysis.Analysis.scatter_plot") as mock_scatter:
+                        with mock.patch("__main__.__builtins__.open", mock.mock_open()) as mock_open:
+                            analysis = Analysis("/tmp")
+                            ret = analysis.generate_report()
+
+                            mock_read_csv.assert_called_once()
+                            mock_ecdf.assert_called_once()
+                            mock_box.assert_called_once()
+                            mock_scatter.assert_called_once()
+                            # we're writing both report.csv and legend.csv
+                            self.assertEqual(mock_open.call_count, 2)
+                            self.assertEqual(ret, 1)
 
     def test_ks_test(self):
         with mock.patch("tlsfuzzer.analysis.pd.read_csv", self.mock_read_csv):
@@ -148,11 +144,6 @@ def setUp(self):
         with mock.patch("tlsfuzzer.analysis.pd.read_csv", mock_read_csv):
             self.analysis = Analysis("/tmp")
 
-    def test_qq_plot(self):
-        with mock.patch("tlsfuzzer.analysis.plt.savefig", mock.Mock()) as mock_save:
-            self.analysis.qq_plot()
-            mock_save.assert_called_once()
-
     def test_ecdf_plot(self):
         with mock.patch("tlsfuzzer.analysis.plt.savefig", mock.Mock()) as mock_save:
             self.analysis.ecdf_plot()
diff --git a/tlsfuzzer/analysis.py b/tlsfuzzer/analysis.py
index 6bc497627..fd1da3c04 100644
--- a/tlsfuzzer/analysis.py
+++ b/tlsfuzzer/analysis.py
@@ -10,7 +10,7 @@
 import sys
 from os.path import join
 from collections import namedtuple
-from itertools import combinations, product
+from itertools import combinations
 
 import numpy as np
 from scipy import stats
@@ -129,42 +129,6 @@ def box_plot(self):
         plt.savefig(join(self.output, "box_plot.png"), bbox_inches="tight")
         plt.close()
 
-    def qq_plot(self):
-        """Generate Q-Q plot grid for the test classes."""
-        indexes = list(range(len(self.class_names)))
-        prod = product(indexes, repeat=2)
-        data_length = len(self.data.iloc[:, 1])
-        quantiles = np.linspace(start=0, stop=1, num=int(data_length))
-
-        fig, axes = plt.subplots(len(indexes),
-                                 len(indexes),
-                                 figsize=(len(indexes) * 3, len(indexes) * 3))
-
-        for index1, index2 in prod:
-            data1 = self.data.iloc[:, index1]
-            data2 = self.data.iloc[:, index2]
-            quantile1 = np.quantile(data1, quantiles, interpolation="midpoint")
-            quantile2 = np.quantile(data2, quantiles, interpolation="midpoint")
-            plot = axes[index1, index2]
-            if index1 == 0:
-                plot.set_title(index2)
-            if index2 == 0:
-                plot.set_ylabel(index1,
-                                fontsize=plt.rcParams['axes.titlesize'])
-            plot.scatter(quantile1, quantile2, marker=".")
-            plot.set_xticks([])
-            plot.set_yticks([])
-            plot.set_xlim([quantile1[0], quantile1[-1]])
-            plot.set_ylim([quantile2[0], quantile2[-1]])
-
-        fig.suptitle("Q-Q plot grid")
-        plt.subplots_adjust(top=0.92,
-                            bottom=0.05,
-                            left=0.1,
-                            right=0.925)
-        plt.savefig(join(self.output, "qq_plot.png"), bbox_inches="tight")
-        plt.close()
-
     def scatter_plot(self):
         """Generate scatter plot showing how the measurement went."""
         plt.figure(figsize=(8, 6))
@@ -209,7 +173,6 @@ def generate_report(self):
         """
         self.box_plot()
         self.scatter_plot()
-        self.qq_plot()
         self.ecdf_plot()
 
         difference = 0

From b4e4fe1d51eeda9e0d37179259ac2277e0b58a5d Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Thu, 9 Jul 2020 18:52:46 +0200
Subject: [PATCH 12/20] analysis: write the general statistics to file, report
 CI

Write the result of the test for uniformity of p-values to a text file

Add also the calculation (using bootstrapping) of the confidence interval
for the truncated mean of differences between the worst pair of samples
(ones that have the smallest p-value from Wilcoxon test).
Write that result to the text file too.
---
 tests/test_tlsfuzzer_analysis.py |   4 +-
 tlsfuzzer/analysis.py            | 126 ++++++++++++++++++++++++++-----
 2 files changed, 108 insertions(+), 22 deletions(-)

diff --git a/tests/test_tlsfuzzer_analysis.py b/tests/test_tlsfuzzer_analysis.py
index 1a073aa00..db1b39626 100644
--- a/tests/test_tlsfuzzer_analysis.py
+++ b/tests/test_tlsfuzzer_analysis.py
@@ -58,7 +58,7 @@ def test_report(self):
                             mock_ecdf.assert_called_once()
                             mock_box.assert_called_once()
                             mock_scatter.assert_called_once()
-                            # we're writing both report.csv and legend.csv
+                            # we're writing to report.csv, and report.txt
                             self.assertEqual(mock_open.call_count, 2)
                             self.assertEqual(ret, 0)
 
@@ -78,7 +78,7 @@ def test_report_neq(self):
                             mock_ecdf.assert_called_once()
                             mock_box.assert_called_once()
                             mock_scatter.assert_called_once()
-                            # we're writing both report.csv and legend.csv
+                            # we're writing to report.csv, and report.txt
                             self.assertEqual(mock_open.call_count, 2)
                             self.assertEqual(ret, 1)
 
diff --git a/tlsfuzzer/analysis.py b/tlsfuzzer/analysis.py
index fd1da3c04..ee3583090 100644
--- a/tlsfuzzer/analysis.py
+++ b/tlsfuzzer/analysis.py
@@ -43,7 +43,8 @@ def main():
 
     if output:
         analysis = Analysis(output)
-        analysis.generate_report()
+        ret = analysis.generate_report()
+        return ret
     else:
         raise ValueError("Missing -o option!")
 
@@ -165,18 +166,38 @@ def make_legend(self):
                    bbox_to_anchor=(0.5, -0.15)
                    )
 
-    def generate_report(self):
+    def calc_diff_conf_int(self, pair, reps=5000, ci=0.95):
         """
-        Compiles a report consisting of statistical tests and plots.
-
-        :return: int 0 if no difference was detected, 1 otherwise
+        Bootstrap a confidence interval for the central tendency of differences
+
+        :param TestPair pair: pairs to calculate the confidence interval
+        :param int reps: how many bootstraping repetitions to perform
+        :param float ci: confidence interval for the low and high estimate.
+            0.95, i.e. "2 sigma", by default
+        :return: tuple with low estimate, median, and high estimate of
+            truncated mean of differences of observations
         """
-        self.box_plot()
-        self.scatter_plot()
-        self.ecdf_plot()
-
+        # because the samples are not independent, we calculate mean of
+        # differences not a difference of means
+        diffs = self.data.iloc[:, pair.index1] - self.data.iloc[:, pair.index2]
+
+        cent_tend = []
+        observ_count = len(diffs)
+
+        for _ in range(reps):
+            boot = np.random.choice(diffs, replace=True, size=observ_count)
+            # use trimmed mean as the pairing of samples in not perfect:
+            # the noise source could get activated in the middle of testing
+            # of the test set, causing some results to be unusable
+            # discard 50% of samples total (cut 25% from the median) to exclude
+            # non central modes
+            cent_tend.append(stats.trim_mean(boot, 0.25))
+
+        return np.quantile(cent_tend, [(1-ci)/2, 0.5, 1-(1-ci)/2])
+
+    def _write_individual_results(self):
+        """Write results to report.csv"""
         difference = 0
-
         # create a report with statistical tests
         box_results = self.box_test()
         wilcox_results = self.wilcoxon_test()
@@ -187,6 +208,8 @@ def generate_report(self):
             writer = csv.writer(file)
             writer.writerow(["Class 1", "Class 2", "Box test",
                              "Wilcoxon signed-rank test"])
+            worst_pair = None
+            worst_p = None
             for pair, result in box_results.items():
                 index1 = pair.index1
                 index2 = pair.index2
@@ -208,15 +231,24 @@ def generate_report(self):
                 if result and wilcox_results[pair] < 0.05:
                     difference = 1
 
+                wilcox_p = wilcox_results[pair]
                 row = [self.class_names[index1],
                        self.class_names[index2],
                        box_write,
-                       wilcox_results[pair]
+                       wilcox_p
                        ]
                 writer.writerow(row)
 
-                p_vals.append(wilcox_results[pair])
+                p_vals.append(wilcox_p)
+
+                if worst_pair is None or wilcox_p < worst_p:
+                    worst_pair = pair
+                    worst_p = wilcox_p
 
+        return difference, p_vals, worst_pair, worst_p
+
+    def _write_legend(self):
+        """Write the legend.csv file."""
         legend_filename = join(self.output, "legend.csv")
         with open(legend_filename, "w") as csv_file:
             writer = csv.writer(csv_file)
@@ -224,16 +256,70 @@ def generate_report(self):
             for num, name in enumerate(self.class_names):
                 writer.writerow([num, name])
 
-        _, p = stats.kstest(p_vals, 'uniform')
-        print("KS-test for uniformity of p-values from Wilcoxon signed-rank "
-              "test")
-        print("p-value: {}".format(p))
-        if p < 0.05:
-            difference = 1
+    def _write_summary(self, difference, p_vals, worst_pair, worst_p):
+        """Write the report.txt file and print summary."""
+        report_filename = join(self.output, "report.csv")
+        text_report_filename = join(self.output, "report.txt")
+        with open(text_report_filename, 'w') as txt_file:
+            _, p = stats.kstest(p_vals, 'uniform')
+            txt = ("KS-test for uniformity of p-values from Wilcoxon "
+                   "signed-rank test")
+            print(txt)
+            txt_file.write(txt)
+            txt_file.write('\n')
+
+            txt = "p-value: {}".format(p)
+            print(txt)
+            txt_file.write(txt)
+            txt_file.write('\n')
+
+            if p < 0.05:
+                difference = 1
+
+            txt = "Worst pair: {}({}), {}({})".format(
+                worst_pair.index1,
+                self.class_names[worst_pair.index1],
+                worst_pair.index2,
+                self.class_names[worst_pair.index2])
+            print(txt)
+            txt_file.write(txt)
+            txt_file.write('\n')
+
+            low, med, high = self.calc_diff_conf_int(worst_pair)
+            # use 95% CI as that translates to 2 standard deviations, making
+            # it easy to estimate higher CIs
+            txt = "Median difference: {:.5e}s, 95% CI: {:.5e}s, {:.5e}s".\
+                format(med, low, high)
+            print(txt)
+            txt_file.write(txt)
+            txt_file.write('\n')
+
+            txt = "For detailed report see {}".format(report_filename)
+            print(txt)
+            txt_file.write(txt)
+            txt_file.write('\n')
+        return difference
+
+    def generate_report(self):
+        """
+        Compiles a report consisting of statistical tests and plots.
+
+        :return: int 0 if no difference was detected, 1 otherwise
+        """
+        self.box_plot()
+        self.scatter_plot()
+        self.ecdf_plot()
+
+        difference, p_vals, worst_pair, worst_p = \
+            self._write_individual_results()
+
+        difference = self._write_summary(difference, p_vals, worst_pair,
+                                         worst_p)
 
-        print("For detailed report see {}".format(report_filename))
         return difference
 
 
 if __name__ == '__main__':
-    main()
+    ret = main()
+    print("Analysis return value: {}".format(ret))
+    sys.exit(ret)

From 945789c0134bb17de90056d13c294b282dd41080 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Fri, 26 Jun 2020 20:44:01 +0200
Subject: [PATCH 13/20] add chapter on running the timing tests

Document how to run the timing tests, why to run them like this and
how to interpret results.

Also move the info from TIMING.md to the main documentation.
---
 TIMING.md                       | 100 +--------
 docs/source/index.rst           |   1 +
 docs/source/timing-analysis.rst | 385 ++++++++++++++++++++++++++++++++
 3 files changed, 390 insertions(+), 96 deletions(-)
 create mode 100644 docs/source/timing-analysis.rst

diff --git a/TIMING.md b/TIMING.md
index 330b2a412..575f88875 100644
--- a/TIMING.md
+++ b/TIMING.md
@@ -1,97 +1,5 @@
-Collecting timing information in `tlsfuzzer`
-===========================================
-
-This document describes how to write tests scenarios that collect timing
-information for later analysis. The timing runner repeatedly runs tests with
-tcpdump in the background. The timing information is only ever extracted from
-the latency of last server response in each conversation. Timing analysis
-requires additional dependencies. You can install them by running this command
-in the root of the repository.
-
-```bash
-pip install -r requirements-timing.txt
-```
-
-Test argument interface
------------------------
-
-Any test that will be collecting timing information need provide the following
-argument interface. Specifying the network interface that packet capture should
-listen on should be enough to time the tests.
-
-| Argument      | Required | Description  |
-|:-------------|:--------:|:------------ |
-| `-i interface`| Yes      | Interface to run tcpdump on       |
-| `-o dir`      | No       | Output directory (default `/tmp`) |
-| `--repeat rep`| No       | Repeat each test `rep` times (default 100) |
-
-Test structure
---------------
-
-After processing these arguments, one would proceed to write the test as usual,
-probably adding a `sanity` test case and tests cases relating to the feature
-under test. The example script `test-conversation.py` can be used as a starting
-point.
-
-After it is clear, that all the tests passed, timing of the tests can be executed.
-Please note that any tests with `sanity` prefix will be ignored in the timing run.
-Start by importing the `TimingRunner` class.
-Because the timing information collection adds some extra dependencies, it is
-necessary to wrap everything related to timing in an if statement:
-
-```python
-if TimingRunner.check_tcpdump():
-```
-
-Now, the the `TimingRunner` class can be initialized with the name of
-the currently run test, list of conversations
-(`sampled_tests` in the reference script),
-output directory (the `-o` argument), TLS server host and port, and finally the
-network interface from the `-i` argument.
-
-Next step is to generate log with random order of test cases for each run. This
-is done by calling the function `generate_log()` from the `TimingRunner`
-instance. This function takes the familiar `run_only` and `run_exclude`
-variables that can filter what tests should be run. Note that this function
-will exclude any tests named "sanity". The last argument to this function is
-how many times each test should be run (`--repeat` argument).
-The log is saved in the output directory.
-
-The last step is to call `run()` function
-from the `TiminingRunner` instance in order to launch tcpdump and begin iterating
-over the tests. Provided you were able to install the timing dependencies,
-this will also launch extraction that will process the packet capture, and output
-the timing information associated with the test class into a csv file, and
-analysis that will generate a report with statistical test results and supporting
-plots.
-
-Executing the test, extraction and analysis
--------------------------------------------
-
-Tests can be executed the same way as any non-timing tests, just make sure the
-current user has permissions to run tcpdump or use sudo. As an example, the
-Bleichenbacher test is extended to use the timing functionality:
-
-```bash
-sudo PYTHONPATH=. python scripts/test-bleichenbacher-workaround.py -i lo
-```
-
-By default, if `dpkt` dependency is available, the extraction will run right
-after the timing packet capture.
-In case you want to run the extraction on another machine (e.g. you were not able
-to install the optional dependencies) you can do this by providing the log, the
-packet capture and server port and hostname (or ip) to the analysis script.
-Resulting file will be outputted to the specified folder.
-
-```bash
-PYTHONPATH=. python tlsfuzzer/extract.py -h localhost -p 4433 -c capture.pcap -l class.log -o /tmp/results/
-```
-
-Timing runner will also launch analysis, if its dependencies are available. Again,
-in case you need to run it later, you can do that by providing the script with an
-output folder where `timing.csv` generated by extraction is present.
-
-```bash
-PYTHONPATH=. python tlsfuzzer/analysis.py -o "/tmp"
-```
+Collecting timing information with `tlsfuzzer`
+==============================================
 
+Running timing tests is described in the [official documentation](
+https://tlsfuzzer.readthedocs.io/en/latest/timing-analysis.html)
diff --git a/docs/source/index.rst b/docs/source/index.rst
index dacacffcd..f1926a275 100644
--- a/docs/source/index.rst
+++ b/docs/source/index.rst
@@ -42,6 +42,7 @@ to see wanted, but not yet implemented features.
    modifying-messages
    connection-state
    statistical-analysis
+   timing-analysis
    ci-integration
    testing-extensions
    testimonials
diff --git a/docs/source/timing-analysis.rst b/docs/source/timing-analysis.rst
new file mode 100644
index 000000000..f0b26be3e
--- /dev/null
+++ b/docs/source/timing-analysis.rst
@@ -0,0 +1,385 @@
+===============
+Timing analysis
+===============
+
+As cryptographic implementations process secret data, they need to ensure
+that side effects of processing that data do not reveal information about
+the secret data.
+
+When an implementation takes different amounts of time to process the messages
+we consider it a timing side-channel. When such side-channels reflect the
+contents of the processed messages we call them timing oracles.
+
+One of the oldest timing oracles is the attack described by Daniel
+Bleichenbacher against RSA key exchange. You can test for it using the
+`test-bleichenbacher-timing.py
+<https://github.com/tomato42/tlsfuzzer/blob/master/scripts/test-bleichenbacher-timing.py>`_
+script.
+
+The other is related to de-padding and verifying MAC values in CBC ciphertexts,
+the newest iteration of which is called Lucky Thirteen. You can test for it
+using the
+`test-lucky13.py
+<https://github.com/tomato42/tlsfuzzer/blob/master/scripts/test-lucky13.py>`_
+script.
+
+Environment setup
+=================
+
+As the scripts measure the time it takes a server to reply to a message,
+a server running alone on a machine, with no interruptions from other
+services or processes will provide statistically significant results with
+fewest observations.
+
+Hardware selection
+------------------
+
+You will want a server with at least 3 physical cores: one to run
+the OS, tlsfuzzer script, etc., one to run the tcpdump process (to ensure
+consistent timestamping of captured packets) and one to run the system under
+test (to ensure consistent response times).
+
+While you can run the tests against a network server, this manual
+doesn't describe how to ensure low latency and low jitter
+to such system under test.
+
+It's better to use a desktop or server system with sufficient cooling as
+thermal throttling is common for laptops running heavy workloads resulting
+in jitter and overall inconsistent results.
+
+OS configuration
+----------------
+
+To ensure the lowest level of noise in measurement, configure the
+system to isolate cores for tcpdump and the system under test.
+
+Red Hat Enterprise Linux 8
+^^^^^^^^^^^^^^^^^^^^^^^^^^
+To isolate CPUs on RHEL-8, install the following packages:
+
+.. code-block::
+
+    dnf install -y tuned tuned-utils tuned-profiles-cpu-partitioning
+
+
+And add the following code to ``/etc/tuned/cpu-partitioning-variables.conf``
+file:
+
+.. code-block::
+
+    isolated_cores=2-10
+    no_balance_cores=2-20
+
+Then apply the profile:
+
+.. code-block::
+
+    tuned-adm profile cpu-partitioning
+
+and restart the system to apply the changes to the kernel.
+
+Then you can install tlsfuzzer dependencies to speed-up the test execution:
+
+.. code-block::
+
+   dnf install python3 python3-devel tcpdump gmp-devel swig mpfr-devel \
+   libmpc openssl-devel make gcc gcc-c++ git libmpc-devel python3-six
+
+   pip3 install m2crypto gmpy gmpy2
+   pip3 install --pre tlslite-ng
+
+And the general requirements to collect and analyse timing results:
+
+.. code-block::
+
+   pip install -r requirements-timing.txt
+
+.. note::
+
+   Because the tests use packet capture to collect timing information and
+   they buffer the messages until all of them have been created, the use
+   of ``m2crypto``, ``gmpy`` and ``gmpy2`` does not have an effect on collected
+   data point, using them will only make tlsfuzzer run the tests at a higher
+   frequency.
+
+Testing theory
+==============
+
+Because the measurements the test performs are statistical by nature,
+the scripts can't just take a mean of observations and compare them with
+means of observations of other tests—that will not provide quantifiable
+results. This is caused by the fact that the measurements don't follow
+a simple and well-defined distribution, in many cases they are
+`multimodal
+<https://en.wikipedia.org/wiki/Multimodal_distribution>`_
+and not `normal<https://en.wikipedia.org/wiki/Normal_distribution>`_.
+That means that the scripts need to use statistical tests to check if the
+observations differ significantly or not.
+
+Most statistical tests work in terms of hypothesis testing.
+The one used in the scripts is called
+`Wilcoxon signed-rank test
+<https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test>`_.
+After executing it against two sets of observations (samples), it outputs
+a "p-value"—a probability of getting such samples, if they were taken from
+populations having the same distribution.
+A high p-value (close to 1) means that the samples likely came from the
+same source while a small value (close to 0, smaller than 0.05) means
+that it's unlikely that they came from the same source distribution.
+
+Generally, script assumes that the p-values below 0.05 mean that the values
+came from different distributions, i.e. the server behaves differently
+for the two provided inputs.
+
+But such small values are expected even if the samples were taken from the same
+distribution if the number of performed tests is large, so you need to check
+if those values are no more common than expected.
+
+If the samples did indeed come from the same population, then the distribution
+of p-values will follow a
+`uniform distribution
+<https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)>`_ with
+values between 0 and 1.
+
+You can use this property to check if not only the failures (small p-values)
+occur not more often than expected, but to check for more general inconsistency
+in p-values (as higher probability of small p-values means that large
+p-values occur less often).
+
+The scripts perform the
+`Kolmogorov–Smirnov test
+<https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test>`_ to test
+the uniformity of p-values of the Wilcoxon tests.
+
+The test scripts allow setting the sample size as it has impact on the smallest
+effect size that the test can detect.
+Generally, with Wilcoxon signed-rank test, the sample size must be proportional
+to 1/e to detect effect of size e.
+That is, to detect a 0.1% difference between expected values of samples, the
+samples must have at least 1000 observations each.
+The actual number depends on multiple factors (including the particular
+samples in question), but it's a good starting point.
+
+Note that this effect size is proportional to magnitude of any single
+observation, at the same time things like size of pre master secret
+or size of MAC are constant, thus configuring the server to use fast ciphers
+and small key sizes for RSA will make the test detect smaller (absolute)
+effect sizes, if they exist.
+
+Finally, the scripts take the pair of samples most dissimilar to each other
+and estimate the difference and the 99% confidence interval for the difference
+to show the estimated effect size.
+
+You can also use the following
+`R
+<https://www.r-project.org/>`_ script to calculate the confidence intervals
+for the difference between a given pair of samples using the Wilcoxon test:
+
+.. code::
+
+   df <- read.csv('timing.csv', header=F)
+   data <- df[,2:length(df[1,])]
+   # print headers (names of tests)
+   df[,1]
+   # run Wilcoxon signed-rank test between second and third sample,
+   # report 99% confidence interval for the difference:
+   wilcox.test(as.numeric(data[2,]), as.numeric(data[3,]), paired=T, conf.int=T, conf.level=0.99)
+
+
+To put into practical terms, a run with 10000 observations, checking a server
+with a 100µs response time will not detect a timing side channel
+that's smaller than 0.01µs (40 cycles on a 4GHz CPU).
+
+Running the tests
+=================
+
+To run the tests:
+
+1. Select a machine with sufficient cooling and a multi-core CPU
+2. Use methods mentioned before to create isolated cores, watch out for
+   hyperthreading
+3. For RSA tests use small key (1024 bit), for CBC tests use a fast cipher and
+   hash.
+4. Start the server on one of the isolated cores, e.g.:
+
+   .. code::
+
+       taskset --cpu-list 2,3 openssl s_server -key key.pem -cert cert.pem -www
+5. Start the test script, provide the IDs of different isolated cores:
+
+   .. code::
+
+       PYTHONPATH=. python3 scripts/test-lucky13.py -i lo --repeat 100 --cpu-list 4,5
+6. Wait (a long) time
+7. Inspect summary of the analysis, or move the test results to a host with
+   newer python and analyse it there.
+
+.. note::
+
+   Since both using pinned cores and collecting packets requires root
+   permissions, execute the previously mentioned commands as root.
+
+.. warning::
+
+   The tests use ``tcpdump`` to collect packets to a file and analyse it
+   later.
+   To process tests with large ``--repeat`` parameter, you need a machine
+   with a large amount of disk space: at least 350MiB with 20 tests at
+   10000 repeats.
+
+
+Test argument interface
+-----------------------
+
+Any test that collects timing information provides the following
+argument interface. Specifying the network interface that packet capture should
+listen on should be enough to time the tests.
+
+================ ========== ==================================================
+ Argument        Required   Description
+================ ========== ==================================================
+``-i interface`` Yes        Interface to run tcpdump on
+``-o dir``       No         Output directory (default ``/tmp``)
+``--repeat rep`` No         Repeat each test ``rep`` times (default 100)
+``--cpu-list``   No         Core IDs to use for running tcpdump (default none)
+================ ========== ==================================================
+
+Executing the test, extraction and analysis
+-------------------------------------------
+
+Tests can be executed the same way as any non-timing tests, just make sure the
+current user has permissions to run tcpdump or use sudo. As an example, the
+Bleichenbacher test is extended to use the timing functionality:
+
+.. code::
+
+   sudo PYTHONPATH=. python scripts/test-bleichenbacher-timing.py -i lo
+
+By default, if ``dpkt`` dependency is available, the extraction will run right
+after the timing packet capture.
+In case you want to run the extraction on another machine (e.g. you were not
+able to install the optional dependencies) you can do this by providing the
+log, the packet capture and server port and hostname (or ip) to the analysis
+script. Resulting file will be outputted to the specified folder.
+
+.. code::
+
+   PYTHONPATH=. python tlsfuzzer/extract.py -h localhost -p 4433 \
+   -c capture.pcap -l class.log -o /tmp/results/
+
+Timing runner will also launch analysis, if its dependencies are available.
+Again, in case you need to run it later, you can do that by providing the
+script with an output folder where extraction step put the ``timing.csv``
+file.
+
+.. code::
+
+   PYTHONPATH=. python tlsfuzzer/analysis.py -o "/tmp"
+
+Interpreting the results
+========================
+
+As mentioned previously, the script executes tests in two stages, one
+is the Wilcoxon signed-rank test between all the samples and then it performs
+a self check on the results of those tests.
+
+If that self test fails, you should inspect the individual test p-values.
+
+If one particular set of tests consistently scores low when compared to
+other tests (e.g. "invalid MAC in Finished on pos 0",
+"invalid MAC in Finished on pos -1" and "invalid padding_length in Finished"
+from ``test-bleichenbacher-timing.py``) but high when compared with each-other,
+that strongly points to a timing side-channel in the system under test.
+
+If the timing signal has a high relative magnitude (one set of tests
+slower than another set by 10%), then you can also use the generated
+``box_plot.png`` graph.
+For small differences with large sample sizes, the differences will be
+statistically detectable, even if not obvious from from the box plot.
+
+Using R you can also generate a graph with median of differences between
+samples, but note that this will take about an hour for 21 tests and
+samples with 1 million observations each on a 4 core/8 thread 2GHz CPU:
+
+.. code::
+
+   library(tidyr)
+   library(ggplot2)
+   library(dplyr)
+   library(data.table)
+   library(boot)
+   df <- fread('timing.csv', header=F)
+   data <- data.frame(t(df[,2:length(df[1,])]))
+   colnames(data) <- as.matrix(df[,1:10])[,1]
+   R = 5000
+   rsq <- function(data, indices) {
+     d <- data[indices]
+     return(mean(d, trim=0.25))
+   }
+   data2 = replicate(R, 0)
+   data2 = cbind(data2)
+   date()
+   for (i in c(2:length(data[1,]))) {
+     a = boot(data[,1]-data[,i], rsq, R=R, parallel="multicore",
+              simple=TRUE, ncpus=8)
+     data2 = cbind(data2, a$t)
+   }
+   date()
+   data2 = data.frame(data2)
+   data2 %>% gather(key="MeasureType", value="Delay") %>%
+   ggplot( aes(x=factor(MeasureType, level=colnames(data2)), y=Delay,
+               fill=factor(MeasureType, level=colnames(data2)))) +
+   geom_violin() + xlab("Test ID") +
+   ylab("Trimmed mean of differences [s]") + labs(fill="Test ID")
+   colnames(data)
+
+
+Writing new test scripts
+========================
+The ``TimingRunner`` repeatedly runs tests with
+``tcpdump`` capturing packets in the background.
+The timing information is then extracted from that ``tcpdump`` capture,
+only the response time to the last client message is extracted from
+the capture.
+
+Test structure
+--------------
+
+After processing these arguments, one would proceed to write the test as usual,
+probably adding a ``sanity`` test case and tests cases relating to the feature
+under test. The example script ``test-conversation.py`` can be used as a
+starting point.
+
+After it is clear, that all the tests passed, timing of the tests can be
+executed.
+Please note that any tests with ``sanity`` prefix will be ignored in the
+timing run.
+Start by importing the ``TimingRunner`` class.
+Because the timing information collection adds some extra dependencies, it is
+necessary to wrap everything related to timing in an if statement:
+
+.. code::
+
+   if TimingRunner.check_tcpdump():
+
+Now, the ``TimingRunner`` class can be initialized with the name of
+the currently run test, list of conversations
+(``sampled_tests`` in the reference scripts),
+output directory (the ``-o`` argument), TLS server host and port, and finally
+the network interface from the ``-i`` argument.
+
+Next step is to generate log with random order of test cases for each run. This
+is done by calling the function ``generate_log()`` from the ``TimingRunner``
+instance. This function takes the familiar ``run_only`` and ``run_exclude``
+variables that can filter what tests should be run. Note that this function
+will exclude any tests named "sanity". The last argument to this function is
+how many times each test should be run (``--repeat`` argument).
+The log is saved in the output directory.
+
+The last step is to call ``run()`` function
+from the ``TiminingRunner`` instance in order to launch tcpdump and begin
+iterating over the tests. Provided you were able to install the timing
+dependencies, this will also launch extraction that will process the packet
+capture, and output the timing information associated with the test class into
+a csv file, and analysis that will generate a report with statistical test
+results and supporting plots.

From 38910caf01bc007611ead22ae57a42980dddb2a8 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Thu, 9 Jul 2020 19:42:56 +0200
Subject: [PATCH 14/20] report to Code Climate run with most code tested

because the code in analysis.py requires dependencies that are not
installed on "clean" 3.6 run, it underreports the code coverage
---
 .travis.yml | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/.travis.yml b/.travis.yml
index 80a3fd411..b3443824c 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -73,7 +73,7 @@ jobs:
    - python: 3.8
      dist: xenial
      sudo: true
-     env: ANALYSIS_DEP=true EXTRACT_DEP=true
+     env: ANALYSIS_DEP=true EXTRACT_DEP=true CC_REPORT=true
 
 branches:
   only:
@@ -114,7 +114,7 @@ install:
   - if [[ $ANALYSIS_DEP == 'true' ]]; then travis_retry pip install -r build-requirements-analysis.txt; fi
   - travis_retry pip install -r requirements.txt
   # codeclimate supports natively just one set of results, so use the most recent python for that
-  - if [[ $TRAVIS_PYTHON_VERSION == '3.6' ]]; then ./cc-test-reporter before-build; fi
+  - if [[ $CC_REPORT == 'true' ]]; then ./cc-test-reporter before-build; fi
 
 script:
   - |
@@ -159,4 +159,4 @@ script:
 
 after_success:
  - travis_retry coveralls
- - if [[ $TRAVIS_PYTHON_VERSION == '3.6' ]]; then coverage xml; ./cc-test-reporter after-build --exit-code $TRAVIS_TEST_RESULT; fi
+ - if [[ $CC_REPORT == 'true' ]]; then coverage xml; ./cc-test-reporter after-build --exit-code $TRAVIS_TEST_RESULT; fi

From 71f29e633aeef178012b549b355f57ad2bad8f8c Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Fri, 10 Jul 2020 18:17:20 +0200
Subject: [PATCH 15/20] analysis: reduce memory use

if we don't specify the data type for the array, pandas without header
uses python object()'s, that translates to gigabytes of memory needed
for processing 20M datapoints.
Coerce them into numpy arrays of float64 objects to significantly reduce
required memory.
---
 tlsfuzzer/analysis.py | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/tlsfuzzer/analysis.py b/tlsfuzzer/analysis.py
index ee3583090..23af4482f 100644
--- a/tlsfuzzer/analysis.py
+++ b/tlsfuzzer/analysis.py
@@ -64,6 +64,10 @@ def load_data(self):
         # transpose and set header
         data = data.transpose()
         data = data.rename(columns=data.iloc[0]).drop(data.index[0])
+        # as we're dealing with 9 digits of precision (nanosecond range)
+        # and the responses can be assumed to take less than a second,
+        # we need to use the double precision IEEE floating point numbers
+        data = data.astype(np.float64)
         return data
 
     def _box_test(self, interval1, interval2, quantile_start, quantile_end):

From 5320e9e706c960d224eaa3cc1581e1162ac8dd91 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Fri, 10 Jul 2020 18:27:06 +0200
Subject: [PATCH 16/20] analysis: actually free memory from matplotlib

Matplotlib *really* wants to keep onto graphs it once drawn, and it's
not really possible to free them (see
https://stackoverflow.com/q/28516828/462370)
so plot them in separate processes, so it has no option but to free
the memory
---
 tests/test_tlsfuzzer_analysis.py | 21 ++++----
 tlsfuzzer/analysis.py            | 86 ++++++++++++++++++++------------
 2 files changed, 65 insertions(+), 42 deletions(-)

diff --git a/tests/test_tlsfuzzer_analysis.py b/tests/test_tlsfuzzer_analysis.py
index db1b39626..5e78f1bae 100644
--- a/tests/test_tlsfuzzer_analysis.py
+++ b/tests/test_tlsfuzzer_analysis.py
@@ -55,9 +55,9 @@ def test_report(self):
                             ret = analysis.generate_report()
 
                             self.mock_read_csv.assert_called_once()
-                            mock_ecdf.assert_called_once()
-                            mock_box.assert_called_once()
-                            mock_scatter.assert_called_once()
+                            #mock_ecdf.assert_called_once()
+                            #mock_box.assert_called_once()
+                            #mock_scatter.assert_called_once()
                             # we're writing to report.csv, and report.txt
                             self.assertEqual(mock_open.call_count, 2)
                             self.assertEqual(ret, 0)
@@ -75,9 +75,9 @@ def test_report_neq(self):
                             ret = analysis.generate_report()
 
                             mock_read_csv.assert_called_once()
-                            mock_ecdf.assert_called_once()
-                            mock_box.assert_called_once()
-                            mock_scatter.assert_called_once()
+                            #mock_ecdf.assert_called_once()
+                            #mock_box.assert_called_once()
+                            #mock_scatter.assert_called_once()
                             # we're writing to report.csv, and report.txt
                             self.assertEqual(mock_open.call_count, 2)
                             self.assertEqual(ret, 1)
@@ -145,17 +145,20 @@ def setUp(self):
             self.analysis = Analysis("/tmp")
 
     def test_ecdf_plot(self):
-        with mock.patch("tlsfuzzer.analysis.plt.savefig", mock.Mock()) as mock_save:
+        with mock.patch("tlsfuzzer.analysis.FigureCanvas.print_figure",
+                        mock.Mock()) as mock_save:
             self.analysis.ecdf_plot()
             mock_save.assert_called_once()
 
     def test_scatter_plot(self):
-        with mock.patch("tlsfuzzer.analysis.plt.savefig", mock.Mock()) as mock_save:
+        with mock.patch("tlsfuzzer.analysis.FigureCanvas.print_figure",
+                        mock.Mock()) as mock_save:
             self.analysis.scatter_plot()
             mock_save.assert_called_once()
 
     def test_box_plot(self):
-        with mock.patch("tlsfuzzer.analysis.plt.savefig", mock.Mock()) as mock_save:
+        with mock.patch("tlsfuzzer.analysis.FigureCanvas.print_figure",
+                        mock.Mock()) as mock_save:
             self.analysis.box_plot()
             mock_save.assert_called_once()
 
diff --git a/tlsfuzzer/analysis.py b/tlsfuzzer/analysis.py
index 23af4482f..72f9ceea4 100644
--- a/tlsfuzzer/analysis.py
+++ b/tlsfuzzer/analysis.py
@@ -8,6 +8,7 @@
 import csv
 import getopt
 import sys
+import multiprocessing as mp
 from os.path import join
 from collections import namedtuple
 from itertools import combinations
@@ -15,9 +16,12 @@
 import numpy as np
 from scipy import stats
 import pandas as pd
-import matplotlib.pyplot as plt
+import matplotlib as mpl
+from matplotlib.figure import Figure
+from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
 
 TestPair = namedtuple('TestPair', 'index1  index2')
+mpl.use('Agg')
 
 
 def help_msg():
@@ -124,47 +128,55 @@ def wilcoxon_test(self):
 
     def box_plot(self):
         """Generate box plot for the test classes."""
-        axes = self.data.plot(kind="box", showfliers=False)
-        axes.set_xticks(range(len(self.data)))
-        axes.set_xticklabels(list(range(len(self.data))))
+        fig = Figure(figsize=(8, 6))
+        canvas = FigureCanvas(fig)
+        ax = fig.add_subplot(1, 1, 1)
 
-        plt.title("Box plot")
-        plt.ylabel("Time [s]")
-        plt.xlabel("Class index")
-        plt.savefig(join(self.output, "box_plot.png"), bbox_inches="tight")
-        plt.close()
+        self.data.boxplot(ax=ax, grid=False, showfliers=False)
+        ax.set_xticks(list(range(len(self.data))))
+        ax.set_xticklabels(list(range(len(self.data))))
+
+        ax.set_title("Box plot")
+        ax.set_ylabel("Time [s]")
+        ax.set_xlabel("Class index")
+        canvas.print_figure(join(self.output, "box_plot.png"),
+                            bbox_inches="tight")
 
     def scatter_plot(self):
         """Generate scatter plot showing how the measurement went."""
-        plt.figure(figsize=(8, 6))
-        plt.plot(self.data, ".", fillstyle='none', alpha=0.6)
-
-        plt.title("Scatter plot")
-        plt.ylabel("Time [s]")
-        plt.xlabel("Sample index")
-        plt.yscale("log")
-        self.make_legend()
-        plt.savefig(join(self.output, "scatter_plot.png"), bbox_inches="tight")
-        plt.close()
+        fig = Figure(figsize=(8, 6))
+        canvas = FigureCanvas(fig)
+        ax = fig.add_subplot(1, 1, 1)
+        ax.plot(self.data, ".", fillstyle='none', alpha=0.6)
+
+        ax.set_title("Scatter plot")
+        ax.set_ylabel("Time [s]")
+        ax.set_xlabel("Sample index")
+        ax.set_yscale("log")
+        self.make_legend(ax)
+        canvas.print_figure(join(self.output, "scatter_plot.png"),
+                            bbox_inches="tight")
 
     def ecdf_plot(self):
         """Generate ECDF plot comparing distributions of the test classes."""
-        plt.figure()
+        fig = Figure(figsize=(8, 6))
+        canvas = FigureCanvas(fig)
+        ax = fig.add_subplot(1, 1, 1)
         for classname in self.data:
             data = self.data.loc[:, classname]
             levels = np.linspace(1. / len(data), 1, len(data))
-            plt.step(sorted(data), levels, where='post')
-        self.make_legend()
-        plt.title("Empirical Cumulative Distribution Function")
-        plt.xlabel("Time [s]")
-        plt.ylabel("Cumulative probability")
-        plt.savefig(join(self.output, "ecdf_plot.png"), bbox_inches="tight")
-        plt.close()
-
-    def make_legend(self):
+            ax.step(sorted(data), levels, where='post')
+        self.make_legend(ax)
+        ax.set_title("Empirical Cumulative Distribution Function")
+        ax.set_xlabel("Time [s]")
+        ax.set_ylabel("Cumulative probability")
+        canvas.print_figure(join(self.output, "ecdf_plot.png"),
+                            bbox_inches="tight")
+
+    def make_legend(self, fig):
         """Generate common legend for plots that need it."""
         header = list(range(len(list(self.data))))
-        plt.legend(header,
+        fig.legend(header,
                    ncol=6,
                    loc='upper center',
                    bbox_to_anchor=(0.5, -0.15)
@@ -310,9 +322,17 @@ def generate_report(self):
 
         :return: int 0 if no difference was detected, 1 otherwise
         """
-        self.box_plot()
-        self.scatter_plot()
-        self.ecdf_plot()
+        # plot in separate processes so that the matplotlib memory leaks are
+        # not cumulative, see https://stackoverflow.com/q/28516828/462370
+        proc = mp.Process(target=self.box_plot)
+        proc.start()
+        proc.join()
+        proc = mp.Process(target=self.scatter_plot)
+        proc.start()
+        proc.join()
+        proc = mp.Process(target=self.ecdf_plot)
+        proc.start()
+        proc.join()
 
         difference, p_vals, worst_pair, worst_p = \
             self._write_individual_results()

From bb8aa8ade7e854575fabfdbb96b501d4b27cc161 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Mon, 13 Jul 2020 16:10:39 +0200
Subject: [PATCH 17/20] bleichenbacher-timing: add test with very short PKCS
 padding

while nominally it's not different from "too short PKCS padding" test,
the number of zero bytes may have an impact on timing of decryption
and thus server response time to malformed CKE messages

Use amount of 0 bytes that is still possible with 1024 bit RSA keys,
but large enough so that any timing side channel will be much more
apparent than with the "too short PKCS padding"
---
 scripts/test-bleichenbacher-timing.py | 29 ++++++++++++++++++++++++++-
 1 file changed, 28 insertions(+), 1 deletion(-)

diff --git a/scripts/test-bleichenbacher-timing.py b/scripts/test-bleichenbacher-timing.py
index 6565f2397..383bcde25 100644
--- a/scripts/test-bleichenbacher-timing.py
+++ b/scripts/test-bleichenbacher-timing.py
@@ -26,7 +26,7 @@
 from tlsfuzzer.utils.lists import natural_sort_keys
 from tlsfuzzer.utils.ordered_dict import OrderedDict
 
-version = 5
+version = 6
 
 
 def help_msg():
@@ -721,6 +721,33 @@ def main():
 
     conversations["too short PKCS padding"] = conversation
 
+    # check if very short PKCS padding doesn't have a different behaviour
+    conversation = Connect(host, port)
+    node = conversation
+    ciphers = [cipher]
+    node = node.add_child(ClientHelloGenerator(ciphers,
+                                               extensions=cln_extensions))
+    node = node.add_child(ExpectServerHello(extensions=srv_extensions))
+
+    node = node.add_child(ExpectCertificate())
+    node = node.add_child(ExpectServerHelloDone())
+    node = node.add_child(TCPBufferingEnable())
+    # move the start of the padding 40 bytes towards LSB
+    subs = {}
+    for i in range(41):
+        subs[i] = 0
+    subs[41] = 2
+    node = node.add_child(ClientKeyExchangeGenerator(padding_subs=subs))
+    node = node.add_child(ChangeCipherSpecGenerator())
+    node = node.add_child(FinishedGenerator())
+    node = node.add_child(TCPBufferingDisable())
+    node = node.add_child(TCPBufferingFlush())
+    node = node.add_child(ExpectAlert(level,
+                                      alert))
+    node.add_child(ExpectClose())
+
+    conversations["very short PKCS padding (40 bytes short)"] = conversation
+
     # check if too long PKCS padding is detected
     conversation = Connect(host, port)
     node = conversation

From 906dd2cbc98dc69c3ede5358f1cf577397d1fdd5 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Mon, 13 Jul 2020 20:59:15 +0200
Subject: [PATCH 18/20] bleichenbacher-timing: document the differences in
 probes

make the test description mention the different groups of tests to
ease debugging and make interpretation of results easier
---
 scripts/test-bleichenbacher-timing.py | 79 +++++++++++++++++++++++++++
 1 file changed, 79 insertions(+)

diff --git a/scripts/test-bleichenbacher-timing.py b/scripts/test-bleichenbacher-timing.py
index 383bcde25..9c104cb8e 100644
--- a/scripts/test-bleichenbacher-timing.py
+++ b/scripts/test-bleichenbacher-timing.py
@@ -840,6 +840,85 @@ def main():
                 failed.append(c_name)
 
     print("Test end")
+    print(20 * '=')
+    print("""Tests for handling of malformed encrypted values in CKE
+
+This test script checks if the server correctly handles malformed
+Client Key Exchange messages in RSA key exchange.
+When executed with `-i` it will also verify that different errors
+are rejected in the same amount of time; it checks for timing
+sidechannel.
+The script executes tests without \"sanity\" in name multiple
+times to estimate server response time.
+
+Quick reminder: when encrypting a value using PKCS#1 v1.5 standard
+the plaintext has the following structure, starting from most
+significant byte:
+- one byte, the version of the encryption, must be 0
+- one byte, the type of encryption, must be 2 (is 1 in case of
+  signature)
+- one or more bytes of random padding, with no zero bytes. The
+  count must equal the byte size of the public key modulus less
+  size of encrypted value and 3 (for version, type and separator)
+  For signatures the bytes must equal 0xff
+- one zero byte that acts as separator between padding and
+  encrypted value
+- one or more bytes that are the encrypted value, for TLS it must
+  be 48 bytes long and the first two bytes need to equal the
+  TLS version advertised in Client Hello
+
+All tests should exhibit the same kind of timing behaviour, but
+if some groups of tests are inconsistent, that points to likely
+place where the timing leak happens:
+- the control group, lack of consistency here points to Lucky 13:
+  - 'invalid MAC in Finished on pos 0'
+  - 'invalid MAC in Finished on pos -1'
+  - 'invalid padding_length in Finished'
+- different PKCS#1 v1.5 padding types, inconsistent results here
+  may be caued by the same code used for decryption and signature
+  verification:
+  - 'set PKCS#1 padding type to 3'
+  - 'set PKCS#1 padding type to 1'
+- incorrect size of encrypted value (pre-master secret),
+  inconsistent results here suggests that the decryption leaks
+  length of plaintext:
+  - 'zero byte in random padding' - this creates a PMS that's 4
+    bytes shorter than the public key size and has a random TLS
+    version
+  - 'zero byte in last byte of random padding' - this creates a
+    PMS that's one byte too long (49 bytes long), with a TLS
+    version that's (0, major_version)
+  - 'no null separator in padding' - as the PMS is all zero, this
+    effectively sends a PMS that's 45 bytes long with TLS version
+    of (0, 0)
+  - 'two byte long PMS (TLS version only)'
+  - 'one byte encrypted value' - the encrypted value is 3, as a
+    correct value for first byte of TLS version
+  - 'too short (47-byte) pre master secret'
+  - 'very short (4-byte) pre master secret'
+  - 'too long (49-byte) pre master secret'
+  - 'very long (124-byte) pre master secret'
+  - 'very long (96-byte) pre master secret'
+- invalid PKCS#1 v1.5 encryption padding:
+  - 'zero byte in first byte of random padding' - this is a mix
+    of too long PMS and invalid padding, it actually doesn't send
+    padding at all, the padding length is zero
+  - 'invalid version number in padding' - this sets the first byte
+    of plaintext to 1
+  - 'no null separator in encrypted value' - this doesn't send a
+    null byte separating padding and encrypted value
+  - 'no encrypted value' - this sends a null separator, but it's
+    the last byte of plaintext
+  - 'too short PKCS padding' - this sends the correct encryption
+    type in the padding (2), but one byte later than required
+  - 'very short PKCS padding (40 bytes short)' - same as above
+    only 40 bytes later
+  - 'too long PKCS padding' this doesn't send the PKCS#1 v1.5
+    version at all, but starts with padding type
+- invalid TLS version in PMS, differences here suggest a leak in
+  code checking for correctness of this value:
+  - 'wrong TLS version (2, 2) in pre master secret'
+  - 'wrong TLS version (0, 0) in pre master secret'""")
     print(20 * '=')
     print("version: {0}".format(version))
     print(20 * '=')

From 057046a8b12c3b8c2fcc1e9133a5a78d0d0a8555 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Mon, 13 Jul 2020 21:13:43 +0200
Subject: [PATCH 19/20] bleichenbacher-timing: add a least synthetic control
 group probe

while the probes that modify the Finished message to create a malformed
ciphertext are very consistent, it also makes them detect Lucky13,
masking the signal from Bleichenbacher.
Use a probe that intentionally causes the master key calculation to use
wrong pre-master key to randomise the plaintext the server will
see on decryption.

Leave old probes in to make combining the results from multiple runs
still possible.
---
 scripts/test-bleichenbacher-timing.py | 35 ++++++++++++++++++++++++---
 1 file changed, 31 insertions(+), 4 deletions(-)

diff --git a/scripts/test-bleichenbacher-timing.py b/scripts/test-bleichenbacher-timing.py
index 9c104cb8e..6397b6e65 100644
--- a/scripts/test-bleichenbacher-timing.py
+++ b/scripts/test-bleichenbacher-timing.py
@@ -26,7 +26,7 @@
 from tlsfuzzer.utils.lists import natural_sort_keys
 from tlsfuzzer.utils.ordered_dict import OrderedDict
 
-version = 6
+version = 7
 
 
 def help_msg():
@@ -293,6 +293,33 @@ def main():
 
     conversations["invalid padding_length in Finished"] = conversation
 
+    # create a CKE with PMS the runner doesn't know/use
+    conversation = Connect(host, port)
+    node = conversation
+    ciphers = [cipher]
+    node = node.add_child(ClientHelloGenerator(ciphers,
+                                               extensions=cln_extensions))
+    node = node.add_child(ExpectServerHello(extensions=srv_extensions))
+
+    node = node.add_child(ExpectCertificate())
+    node = node.add_child(ExpectServerHelloDone())
+    node = node.add_child(TCPBufferingEnable())
+    # use too short PMS but then change padding so that the PMS is
+    # correct length with correct TLS version but the encryption keys
+    # that tlsfuzzer calculates will be incorrect
+    node = node.add_child(ClientKeyExchangeGenerator(
+        padding_subs={-3: 0, -2: 3, -1: 3},
+        premaster_secret=bytearray([1] * 46)))
+    node = node.add_child(ChangeCipherSpecGenerator())
+    node = node.add_child(FinishedGenerator())
+    node = node.add_child(TCPBufferingDisable())
+    node = node.add_child(TCPBufferingFlush())
+    node = node.add_child(ExpectAlert(level,
+                                      alert))
+    node.add_child(ExpectClose())
+
+    conversations["fuzzed pre master secret"] = conversation
+
     # set 2nd byte of padding to 3 (invalid value)
     conversation = Connect(host, port)
     node = conversation
@@ -873,9 +900,9 @@ def main():
 - the control group, lack of consistency here points to Lucky 13:
   - 'invalid MAC in Finished on pos 0'
   - 'invalid MAC in Finished on pos -1'
-  - 'invalid padding_length in Finished'
-- different PKCS#1 v1.5 padding types, inconsistent results here
-  may be caued by the same code used for decryption and signature
+  - 'fuzzed pre master secret' - this will end up with random
+    plaintexts in record with Finished, most resembling a randomly
+    selected PMS by the server
   verification:
   - 'set PKCS#1 padding type to 3'
   - 'set PKCS#1 padding type to 1'

From c97aebe1fee58a57623e0b87a3d464de794b72f0 Mon Sep 17 00:00:00 2001
From: Hubert Kario <hkario@redhat.com>
Date: Tue, 14 Jul 2020 13:25:57 +0200
Subject: [PATCH 20/20] analysis: write legend for graphs

---
 tests/test_tlsfuzzer_analysis.py | 10 ++++++----
 tlsfuzzer/analysis.py            |  1 +
 2 files changed, 7 insertions(+), 4 deletions(-)

diff --git a/tests/test_tlsfuzzer_analysis.py b/tests/test_tlsfuzzer_analysis.py
index 5e78f1bae..8c25d28c4 100644
--- a/tests/test_tlsfuzzer_analysis.py
+++ b/tests/test_tlsfuzzer_analysis.py
@@ -58,8 +58,9 @@ def test_report(self):
                             #mock_ecdf.assert_called_once()
                             #mock_box.assert_called_once()
                             #mock_scatter.assert_called_once()
-                            # we're writing to report.csv, and report.txt
-                            self.assertEqual(mock_open.call_count, 2)
+                            # we're writing to report.csv, legend.csv, and
+                            # report.txt
+                            self.assertEqual(mock_open.call_count, 3)
                             self.assertEqual(ret, 0)
 
     def test_report_neq(self):
@@ -78,8 +79,9 @@ def test_report_neq(self):
                             #mock_ecdf.assert_called_once()
                             #mock_box.assert_called_once()
                             #mock_scatter.assert_called_once()
-                            # we're writing to report.csv, and report.txt
-                            self.assertEqual(mock_open.call_count, 2)
+                            # we're writing to report.csv, legend.csv,
+                            # and report.txt
+                            self.assertEqual(mock_open.call_count, 3)
                             self.assertEqual(ret, 1)
 
     def test_ks_test(self):
diff --git a/tlsfuzzer/analysis.py b/tlsfuzzer/analysis.py
index 72f9ceea4..f6a0c6a97 100644
--- a/tlsfuzzer/analysis.py
+++ b/tlsfuzzer/analysis.py
@@ -333,6 +333,7 @@ def generate_report(self):
         proc = mp.Process(target=self.ecdf_plot)
         proc.start()
         proc.join()
+        self._write_legend()
 
         difference, p_vals, worst_pair, worst_p = \
             self._write_individual_results()
